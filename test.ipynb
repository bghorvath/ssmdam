{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/biagio/.cache/pypoetry/virtualenvs/acoustic-anomaly-detection-hYgSvyc4-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torchaudio\n",
    "from transformers import AutoProcessor, ASTModel\n",
    "from torchaudio.transforms import MelSpectrogram, MFCC\n",
    "\n",
    "from acoustic_anomaly_detection.dataset import AudioDataset\n",
    "\n",
    "params = yaml.safe_load(open(\"params.yaml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "ast = AutoProcessor.from_pretrained(\n",
    "    \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "from acoustic_anomaly_detection.utils import get_attributes\n",
    "\n",
    "class ASTProcessor(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ast = AutoProcessor.from_pretrained(\n",
    "            \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.ast(\n",
    "            x.squeeze(0),\n",
    "            sampling_rate=params[\"transform\"][\"params\"][\"sr\"],\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return x[\"input_values\"]\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_list: list,\n",
    "    ) -> None:\n",
    "        self.file_list = file_list\n",
    "        self.seed = params[\"train\"][\"seed\"]\n",
    "        self.data_sources = params[\"data\"][\"data_sources\"]\n",
    "        self.transform_type = params[\"transform\"][\"type\"]\n",
    "        self.segment = params[\"transform\"][\"segment\"]\n",
    "        self.sr = params[\"transform\"][\"params\"][\"sr\"]\n",
    "        self.duration = params[\"transform\"][\"params\"][\"duration\"]\n",
    "        self.length = self.sr * self.duration\n",
    "        self.transform_func = ASTProcessor()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def cut(self, signal: torch.Tensor) -> torch.Tensor:\n",
    "        if signal.shape[1] > self.length:\n",
    "            signal = signal[:, : self.length]\n",
    "        elif signal.shape[1] < self.length:\n",
    "            signal = torch.nn.functional.pad(signal, (0, self.length - signal.shape[1]))\n",
    "        return signal\n",
    "\n",
    "    def resample(self, signal: torch.Tensor, sr: int) -> torch.Tensor:\n",
    "        if sr != self.sr:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.sr)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def mix_down(self, signal: torch.Tensor) -> torch.Tensor:\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "    \n",
    "    def transform(self, signal: torch.Tensor, sr: int) -> torch.Tensor:\n",
    "        signal = self.resample(signal, sr)\n",
    "        signal = self.mix_down(signal)\n",
    "        signal = self.cut(signal)\n",
    "        signal = self.transform_func(signal)\n",
    "        return signal\n",
    "    \n",
    "    def __getitem__(self, idx) -> tuple[torch.Tensor, dict[str, str]]:\n",
    "        file_path = self.file_list[idx]\n",
    "        signal, sr = torchaudio.load(file_path)\n",
    "        attributes = get_attributes(os.path.basename(file_path))\n",
    "        return self.transform(signal, sr), attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")#, max_length=998, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "from torcheval.metrics.functional import binary_auroc, binary_auprc\n",
    "\n",
    "from acoustic_anomaly_detection.utils import slide_window, reverse_slide_window\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, model_name: str, input_size: int):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        window_size = params[\"transform\"][\"params\"][\"window_size\"]\n",
    "        stride = params[\"transform\"][\"params\"][\"stride\"]\n",
    "        self.input_size = ((input_size - window_size) // stride + 1) * window_size\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: tuple[torch.Tensor, dict[str, str]], batch_idx: int\n",
    "    ) -> torch.Tensor:\n",
    "        x, _ = batch\n",
    "        x = nn.Flatten(0, 1)(x)\n",
    "        x_hat = self(x)\n",
    "        loss = nn.functional.mse_loss(x_hat, x)\n",
    "        self.log(\n",
    "            f\"{self.model_name}_train_loss\",\n",
    "            loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: tuple[torch.Tensor, dict[str, str]], batch_idx: int\n",
    "    ) -> torch.Tensor:\n",
    "        x, _ = batch\n",
    "        x = nn.Flatten(0, 1)(x)\n",
    "        x_hat = self(x)\n",
    "        loss = nn.functional.mse_loss(x_hat, x)\n",
    "        self.log(\n",
    "            f\"{self.model_name}_val_loss\",\n",
    "            loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def test_step(\n",
    "        self, batch: tuple[torch.Tensor, dict[str, str]], batch_idx: int\n",
    "    ) -> None:\n",
    "        x, attributes = batch\n",
    "        x = nn.Flatten(0, 1)(x)\n",
    "        x_hat = self(x)\n",
    "        error_score = torch.mean(torch.square(x_hat - x))\n",
    "        self.error_score.append(error_score.item())\n",
    "        y = 1 if attributes[\"label\"] == \"anomaly\" else 0\n",
    "        self.y.append(y)\n",
    "\n",
    "    def on_test_epoch_start(self) -> None:\n",
    "        self.error_score = []\n",
    "        self.y = []\n",
    "\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        error_score = torch.tensor(self.error_score)\n",
    "        y = torch.tensor(self.y)\n",
    "        auroc = binary_auroc(error_score, y)\n",
    "        auprc = binary_auprc(error_score, y)\n",
    "        self.log(f\"{self.model_name}_auroc_epoch\", auroc, prog_bar=True, logger=True)\n",
    "        self.log(f\"{self.model_name}_auprc_epoch\", auprc, prog_bar=True, logger=True)\n",
    "\n",
    "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
    "        return torch.optim.Adam(self.parameters(), lr=params[\"train\"][\"lr\"])\n",
    "\n",
    "\n",
    "class SimpleAE(Model):\n",
    "    def __init__(self, model_name: str, input_size: int) -> None:\n",
    "        super().__init__(model_name)\n",
    "        self.encoder_layers = params[\"model\"][\"layers\"][\"encoder\"]\n",
    "        self.decoder_layers = params[\"model\"][\"layers\"][\"decoder\"]\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, self.encoder_layers[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.encoder_layers[0], self.encoder_layers[1]),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.decoder_layers[0], self.decoder_layers[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.decoder_layers[1], input_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        z = slide_window(x)\n",
    "        z = nn.Flatten(-2, -1)(z)\n",
    "        z = self.encoder(z)\n",
    "        z = self.decoder(z)\n",
    "        z = reverse_slide_window(z)\n",
    "        return z.view(x.shape)\n",
    "\n",
    "\n",
    "class BaselineAE(Model):\n",
    "    \"\"\"\n",
    "    Baseline AE model\n",
    "    Source: https://github.com/nttcslab/dcase2023_task2_baseline_ae/blob/main/networks/dcase2023t2_ae/network.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str, input_size: int) -> None:\n",
    "        super().__init__(model_name, input_size)\n",
    "        self.encoder_layers = params[\"model\"][\"layers\"][\"encoder\"]\n",
    "        self.decoder_layers = params[\"model\"][\"layers\"][\"decoder\"]\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.encoder_layers[0]),\n",
    "            nn.BatchNorm1d(self.encoder_layers[0], momentum=0.01, eps=1e-03),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.encoder_layers[0], self.encoder_layers[1]),\n",
    "            nn.BatchNorm1d(self.encoder_layers[1], momentum=0.01, eps=1e-03),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.encoder_layers[1], self.encoder_layers[2]),\n",
    "            nn.BatchNorm1d(self.encoder_layers[2], momentum=0.01, eps=1e-03),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.encoder_layers[2], self.encoder_layers[3]),\n",
    "            nn.BatchNorm1d(self.encoder_layers[3], momentum=0.01, eps=1e-03),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.encoder_layers[3], self.encoder_layers[4]),\n",
    "            nn.BatchNorm1d(self.encoder_layers[4], momentum=0.01, eps=1e-03),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.decoder_layers[0], self.decoder_layers[1]),\n",
    "            nn.BatchNorm1d(self.decoder_layers[1], momentum=0.01, eps=1e-03),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.decoder_layers[1], self.decoder_layers[2]),\n",
    "            nn.BatchNorm1d(self.decoder_layers[2], momentum=0.01, eps=1e-03),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.decoder_layers[2], self.decoder_layers[3]),\n",
    "            nn.BatchNorm1d(self.decoder_layers[3], momentum=0.01, eps=1e-03),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.decoder_layers[3], self.decoder_layers[4]),\n",
    "            nn.BatchNorm1d(self.decoder_layers[4], momentum=0.01, eps=1e-03),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.decoder_layers[4], self.input_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        z = slide_window(x)\n",
    "        z = nn.Flatten(-2, -1)(z)\n",
    "        z = self.encoder(z)\n",
    "        z = self.decoder(z)\n",
    "        z = reverse_slide_window(z)\n",
    "        return z.view(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.rand(309, 16, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([309, 16, 128])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dir = os.path.join(\"data\", \"raw\", \"dcase2023t2\", \"dev\", \"bearing\", \"train\")\n",
    "file_list = [\n",
    "    os.path.join(audio_dir, file)\n",
    "    for file in os.listdir(audio_dir)\n",
    "]\n",
    "\n",
    "dataset = AudioDataset(\n",
    "    file_list=file_list,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    num_workers=8,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "input_size = dataset[0][0].shape[1:].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineAE(\"baseline\", input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 128, 313])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    x, attributes = batch\n",
    "    #x = nn.Flatten(0, 1)(x)\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([309, 2, 128, 5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = slide_window(x)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = nn.Flatten(-2, -1)(z)\n",
    "# z = self.encoder(z)\n",
    "# z = self.decoder(z)\n",
    "# z = reverse_slide_window(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1024, 128])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([124, 2, 1024, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([618, 640])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = nn.Flatten(-2, -1)(z)\n",
    "z = nn.Flatten(0, 1)(z)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reverse_slide_window(decoded_tensor: torch.Tensor) -> torch.Tensor:\n",
    "# Infer shapes and parameters\n",
    "\n",
    "decoded_tensor = z\n",
    "\n",
    "total_windows, flattened_feature_dim = decoded_tensor.shape\n",
    "window_size = params[\"transform\"][\"params\"][\"window_size\"]\n",
    "feature_dim = flattened_feature_dim // window_size\n",
    "center_idx = window_size // 2  # this should be 2\n",
    "batch_size = params[\"train\"][\"batch_size\"]\n",
    "\n",
    "# Reshape the tensor to [batch_size, 309, feature_dim, window_size]\n",
    "windows = total_windows // batch_size\n",
    "reshaped_tensor = decoded_tensor.view(batch_size, windows, feature_dim, window_size)\n",
    "\n",
    "# Take the nth value from all windows\n",
    "center_values = reshaped_tensor[:, :, :, center_idx]\n",
    "\n",
    "# Take 2 leftmost values from the first window and 2 rightmost values from the last window\n",
    "left_values = reshaped_tensor[:, 0, :, :2]\n",
    "right_values = reshaped_tensor[:, -1, :, -2:]\n",
    "\n",
    "# Concatenate everything to reconstruct\n",
    "reconstructed = torch.cat([left_values, center_values, right_values], dim=1)\n",
    "\n",
    "# return reconstructed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:\n",
      "\n",
      "Shape: torch.Size([2, 313, 128])\n",
      "\n",
      "Windowed Tensor:\n",
      "\n",
      "Shape: torch.Size([2, 309, 5, 128])\n",
      "\n",
      "Reconstructed Tensor:\n",
      "\n",
      "Shape: torch.Size([2, 313, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Sample 3D tensor: 5 batches of time sequences of length 10, with 3 features\n",
    "tensor = torch.arange(80128).view(2, 128, 313)\n",
    "tensor = tensor.transpose(1, 2)\n",
    "print(\"Original Tensor:\")\n",
    "print(f\"\\nShape: {tensor.shape}\")\n",
    "#print(tensor)\n",
    "\n",
    "def slide_window_3d(signal: torch.Tensor, window_size=5, stride=1) -> torch.Tensor:\n",
    "    batch_size, length, feature_size = signal.shape\n",
    "    num_windows = (length - window_size) // stride + 1\n",
    "    windows = []\n",
    "    for i in range(num_windows):\n",
    "        window = signal[:, i * stride : i * stride + window_size, :]\n",
    "        windows.append(window)\n",
    "    return torch.stack(windows, dim=1)\n",
    "\n",
    "def reverse_slide_window_3d(windowed_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    batch_size, num_windows, window_size, feature_size = windowed_tensor.shape\n",
    "    center_idx = window_size // 2  # e.g., 2 for window size of 5\n",
    "    \n",
    "    # Take the nth value from all windows for all features\n",
    "    center_values = windowed_tensor[:, :, center_idx, :]\n",
    "    \n",
    "    # Take the leftmost values from the first window for all batches and features\n",
    "    left_values = windowed_tensor[:, 0, :center_idx, :]\n",
    "    \n",
    "    # Take the rightmost values from the last window for all batches and features\n",
    "    right_values = windowed_tensor[:, -1, center_idx+1:, :]\n",
    "    \n",
    "    # Concatenate everything to reconstruct for each batch and feature\n",
    "    reconstructed = torch.cat([left_values, center_values, right_values], dim=1)\n",
    "    \n",
    "    return reconstructed\n",
    "\n",
    "# Apply sliding window\n",
    "windowed_tensor = slide_window_3d(tensor)\n",
    "print(\"\\nWindowed Tensor:\")\n",
    "print(f\"\\nShape: {windowed_tensor.shape}\")\n",
    "#print(windowed_tensor)\n",
    "\n",
    "# Apply reverse sliding window\n",
    "reconstructed_tensor = reverse_slide_window_3d(windowed_tensor)\n",
    "print(\"\\nReconstructed Tensor:\")\n",
    "print(f\"\\nShape: {reconstructed_tensor.shape}\")\n",
    "#print(reconstructed_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acoustic-anomaly-detection-hYgSvyc4-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
